import memos
from memos.configs.embedder import EmbedderConfigFactory
from memos.configs.memory import TreeTextMemoryConfig
from memos.configs.mem_reader import SimpleStructMemReaderConfig
from memos.embedders.factory import EmbedderFactory
from memos.mem_reader.simple_struct import SimpleStructMemReader
from memos.memories.textual.tree import TreeTextMemory
import ast
from dotenv import load_dotenv
from memos.mem_cube.general import GeneralMemCube
from memos.configs.mem_cube import GeneralMemCubeConfig
from memos.memories.textual.item import TextualMemoryItem, TreeNodeTextualMemoryMetadata
import memos.memories.textual.tree_text_memory.retrieve.searcher as searcher
import requests
import json
import os
import pickle
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from memos.configs.mem_os import MOSConfig
import inspect
from memos.configs.embedder import EmbedderConfigFactory
import uuid
from memos.mem_os.main import MOS
from memos.llms.openai import OpenAILLM
from memos.configs.llm import OpenAILLMConfig
from pathlib import Path
from memos.memories.textual.tree_text_memory.organize import manager

def safe_del(self):
    try:
        if hasattr(self, 'close') and callable(self.close):
            self.close()
    except Exception as e:
        print(f"[MonkeyPatch] __del__ failed safely: {e}")

# Monkey patch
manager.MemoryManager.__del__ = safe_del

def get_memcube_config():
    print("ğŸ”§ åˆ›å»ºMemCubeé…ç½® (APIç‰ˆ)...")

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # æ£€æŸ¥APIé…ç½®
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")

    if not openai_key:
        raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OpenAI APIå¯†é’¥ã€‚")

    print("âœ… æ£€æµ‹åˆ°OpenAI APIæ¨¡å¼")

    # è·å–é…ç½®
    user_id = os.getenv("MOS_USER_ID", "default_user")
    top_k = int(os.getenv("MOS_TOP_K", "5"))

    # OpenAIæ¨¡å¼é…ç½®
    config_memcube = GeneralMemCubeConfig(
        user_id=user_id,
        cube_id=f"{user_id}_structured_memories_cube",
        text_mem={
            "backend": "general_text",
            "config": {
                "extractor_llm": {
                    "backend": "openai",
                    "config": {
                        "model_name_or_path": "gpt-4o",
                        "api_key": openai_key,
                        "api_base": openai_base,
                        "temperature": 0.8,
                        "max_tokens": 8192,
                    }
                },
                "embedder": {
                        "backend": "universal_api",
                        "config": {
                            "provider": "openai",
                            "api_key": openai_key,
                            "model_name_or_path": "text-embedding-ada-002",
                            "base_url": openai_base,
                        }
                },
                "vector_db": {
                    "backend": "qdrant",
                    "config": {
                        "collection_name": f"{user_id}_structured_memories",
                        "vector_dimension": 1536,
                        "distance_metric": "cosine"
                    }
                }
            }
        },
        act_mem={"backend": "uninitialized"},
        para_mem={"backend": "uninitialized"}
    )
    return config_memcube

def get_following_memory_texts(memory: TreeTextMemory, start_id: str, k: int = 30) -> list[str]:
    """
    Return the metadata["memory"] strings of the next k nodes following a given node via FOLLOWS edges.

    Args:
        memory (TreeTextMemory): Memory system instance.
        start_id (str): The starting node ID.
        k (int): Number of following nodes to retrieve.

    Returns:
        list[str]: List of memory texts from the following nodes.
    """
    graph = memory.graph_store.export_graph()
    nodes = {node["id"]: node for node in graph["nodes"]}
    follows_map = {
        edge["source"]: edge["target"]
        for edge in graph["edges"]
        if edge["type"] == "FOLLOWS"
    }

    result = []
    current_id = start_id
    for _ in range(k):
        next_id = follows_map.get(current_id)
        if not next_id or next_id not in nodes:
            break

        metadata = nodes[next_id].get("metadata", {})
        memory_text = metadata.get("memory") or nodes[next_id].get("memory")  # fallback
        if memory_text:
            result.append(memory_text)
        current_id = next_id

    return result

def key_event_extraction(query,llm):
    name_prompt = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªç²¾å‡†äº‹ä»¶æŠ½å–å™¨ã€‚ç”¨æˆ·ä¼šæè¿°ä¸€ä¸ªæˆ–å¤šä¸ªå°è¯´ä¸­å‘ç”Ÿè¿‡çš„äº‹ä»¶ï¼Œä½ éœ€è¦ä»ä¸­æå–å‡ºç”¨æˆ·æƒ³è¦æ”¹å˜æˆ–è®¨è®ºçš„å…³é”®äº‹ä»¶ï¼Œå¹¶ç”¨ä¸€å¥è¯ç®€æ´æè¿°æ¯ä¸ªäº‹ä»¶ã€‚ä»…æ¦‚æ‹¬äº‹ä»¶ï¼Œæ— éœ€æ»¡è¶³ç”¨æˆ·éœ€æ±‚\n"
                        "è¦æ±‚ï¼š\n"
                        "1. æ¯ä¸ªäº‹ä»¶å¿…é¡»æ˜¯çœŸå®å‘ç”Ÿåœ¨å°è¯´åŸæ–‡ä¸­çš„äº‹ä»¶ï¼Œè€Œéå‡è®¾ã€‚\n"
                        "2. æ¯ä¸ªäº‹ä»¶å¿…é¡»ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ„æˆ Python list çš„å…ƒç´ ã€‚\n"
                        "3. æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯åˆæ³•çš„ Python listï¼Œä¾‹å¦‚ï¼š\n"
                        '''["ä¹”å³°è¯¯æ€é˜¿æœ±", "æ®µèª‰è·³å´–é€ƒé¿å©šå§»"]\n'''
                        "ä½ åªè¾“å‡ºè¿™ä¸ª listï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–çš„å†…å®¹ã€‚"
        },
        {
            "role": "user",
            "content": query
        }
    ]
    key_event = llm.generate(name_prompt)

    return ast.literal_eval(key_event)
    
def refine_command(query: str,llm) -> str:
    name_prompt = [
        {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡æŒ‡ä»¤ä¼˜åŒ–å™¨ï¼Œä¸“ç”¨äºå°è¯´ç±»ç”¨æˆ·ä»»åŠ¡ã€‚\n"
                "ç”¨æˆ·ä¼šç»™å‡ºä¸€ä¸ªéšæ„ã€æ¨¡ç³Šã€ç®€çŸ­æˆ–ä¸å®Œæ•´çš„è¯·æ±‚ï¼Œ\n"
                "ä½ éœ€è¦å°†å®ƒè¡¥å…¨ä¸ºä¸€æ¡å®Œæ•´ã€æ¸…æ™°ã€ç²¾ç‚¼çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚\n\n"
                "æŒ‡ä»¤å†…å®¹å¯ä»¥åŒ…æ‹¬ä½†ä¸é™äºï¼š\n"
                "1. å°è¯´å‰§æƒ…ç»­å†™ï¼ˆå¦‚æ¨¡ä»¿é‡‘åº¸é£æ ¼ç»­å†™ä¸€æ®µä¸­æ®µå‰§æƒ…ï¼‰\n"
                "2. å°è¯´äººç‰©å¯¹è¯ï¼ˆå¦‚â€œè¯·æ¨¡æ‹Ÿæ®µèª‰ä¸ç‹è¯­å«£çš„ä¸€æ®µå¯¹è¯â€ï¼‰\n"
                "3. å‰§æƒ…åˆ†æï¼ˆå¦‚â€œåˆ†æä¹”å³°è¯¯æ€é˜¿æœ±åäººç‰©å¿ƒç†ä¸æƒ…èŠ‚å½±å“â€ï¼‰\n"
                "4. ä¸–ç•Œè§‚è®¾å®šè§£è¯»ï¼ˆå¦‚â€œè§£é‡Šè§è¿œå±±å’Œç„æ…ˆä¹‹é—´çš„æ©æ€¨â€ï¼‰\n"
                "5. å¤šè§’è‰²åšå¼ˆå…³ç³»æ¢³ç†ï¼ˆå¦‚â€œç®€æè§å³°ã€æ…•å®¹å¤ã€æ®µèª‰ä¸‰äººçš„ç«‹åœºå†²çªâ€ï¼‰\n\n"
                "ä½ åªéœ€è¾“å‡ºæœ€ç»ˆè¡¥å…¨åçš„æ¸…æ™°è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–å¼•å¯¼æ–‡å­—ã€‚\n"
                "å¦‚æœåŸå§‹è¾“å…¥éå¸¸æ¨¡ç³Šï¼Œæ¯”å¦‚â€˜ç»§ç»­â€™ã€â€˜å¯¹è¯â€™ï¼Œä½ éœ€è¦æ ¹æ®å°è¯´ä¸Šä¸‹æ–‡è¡¥å…¨ã€‚\n\n"
                "ã€ç¤ºä¾‹1ã€‘\n"
                "è¾“å…¥ï¼šâ€˜å¦‚æœé˜¿æœ±æ²¡æ­»å‘¢â€™\n"
                "è¾“å‡ºï¼šâ€˜è¯·å‡è®¾é˜¿æœ±æœªæ­»ï¼Œæ¨¡ä»¿é‡‘åº¸é£æ ¼ç»­å†™ä¸€æ®µå®Œæ•´ä¸­æ®µå‰§æƒ…ã€‚â€™\n\n"
                "ã€ç¤ºä¾‹2ã€‘\n"
                "è¾“å…¥ï¼šâ€˜ä¹”å³°å’Œè™šç«¹çš„å…³ç³»â€™\n"
                "è¾“å‡ºï¼šâ€˜è¯·åˆ†æä¹”å³°ä¸è™šç«¹ä¹‹é—´çš„å…„å¼Ÿå…³ç³»æ¼”å˜ï¼Œç»“åˆå‰§æƒ…å˜åŒ–å’Œäººç‰©å¿ƒç†è¿›è¡Œæ·±å…¥å‰–æã€‚â€™\n\n"
                "ã€ç¤ºä¾‹3ã€‘\n"
                "è¾“å…¥ï¼šâ€˜ç»§ç»­â€™\n"
                "è¾“å‡ºï¼šâ€˜ç»§ç»­å‰æ–‡çš„å°è¯´å‰§æƒ…ï¼Œæ¨¡ä»¿é‡‘åº¸é£æ ¼ç»­å†™ä¸€æ®µä¸­æ®µæƒ…èŠ‚â€™"
            )
        },
        {
            "role": "user",
            "content": query
        }
    ]
    return llm.generate(name_prompt)


def get_event_contexts_for_prompt(
    memory: TreeTextMemory,
    event_texts: list[str],
    k: int = 30
) -> dict[str, list[str]]:
    """
    å¯¹æ¯ä¸ªäº‹ä»¶æ‰§è¡Œ search + æ‹¿å‰ä¸¤ä¸ªåŒ¹é…ç‚¹ + è·å–åç»­å‰§æƒ…ï¼Œç”¨äºæ„é€  GPT promptã€‚
    
    Args:
        memory: TreeTextMemory å®ä¾‹
        event_texts: æå–å‡ºçš„äº‹ä»¶æ–‡æœ¬åˆ—è¡¨
        k: æ¯ä¸ªèŠ‚ç‚¹å‘åå–å‡ ä¸ª follows

    Returns:
        dict[str, list[str]]: {event_text -> [åç»­memory strings]}
    """
    result = {}

    for event in event_texts:
        try:
            matches = memory.search(event, top_k=2)
            memory_strings = []

            for match in matches:
                follow_texts = get_following_memory_texts(memory, match.id, k)
                memory_strings.extend(follow_texts)

            result[event] = memory_strings

        except Exception as e:
            print(f"Error processing event '{event}': {e}")
            result[event] = []

    return result


def node_dict_to_textual_item(node_dict):
    return TextualMemoryItem(
        id=node_dict["id"],
        memory=node_dict["memory"],
        metadata=TreeNodeTextualMemoryMetadata(**node_dict["metadata"])
    )


def get_embedding(text):
    url = "http://123.129.219.111:3000/v1/embeddings"
    headers = {
        "Authorization": "Bearer sk-BboZZQNg570YPNhJrGjyPIjOsBpCzUSHRZaDFv4BBVCqTkRQ",
        "Content-Type": "application/json"
    }
    payload = {
        "input": text,
        "model": "text-embedding-ada-002"
    }
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()["data"][0]["embedding"]
    except Exception as e:
        print(f"âš ï¸ è·å– embedding å¤±è´¥ï¼š{e}")
        return None
# === TIME STAMP ===
def iso_now():
    return datetime.now().isoformat()

# === CREATE MEMORY NODE ===
def create_memory_node_working(content, entities, key, memory_type="WorkingMemory"):
    now = iso_now()
    node_id = str(uuid.uuid4())
    embedding = get_embedding(content)

    metadata = TreeNodeTextualMemoryMetadata(
        user_id="",
        session_id="",
        status="activated",
        type="fact",
        confidence=0.99,
        entities=entities,
        tags=["äº‹ä»¶"] if "äº‹ä»¶" in key else ["å…³ç³»"],
        updated_at=now,
        memory_type=memory_type,
        key=key,
        sources=[],
        embedding=embedding,
        created_at=now,
        usage=[],
        background=""
    )

    return TextualMemoryItem(id=node_id, memory=content, metadata=metadata)

def build_story_engine_system_prompt(past_event) -> str:
    return (
        "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£å°è¯´åˆ›ä½œçš„é«˜çº§ AI æ¨¡å‹ï¼Œæ“…é•¿ä»¥æ¨¡ä»¿åŸä½œè€…é£æ ¼åˆ›ä½œä¸­æ®µæƒ…èŠ‚ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·è¾“å…¥çš„å‡è®¾å‰§æƒ…å’Œäººç‰©è®°å¿†ï¼ˆmemoryï¼‰ï¼Œåˆ›ä½œä¸€æ®µå®Œæ•´çš„å‰§æƒ…å‘å±•,å¤§çº¦2000å­—ã€‚\n\n"
        "ä½ çš„åˆ›ä½œå¿…é¡»éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š\n\n"
        "1. ä½¿ç”¨åŸæœ¬é£æ ¼çš„æ®µè½å¼å°è¯´è¯­è¨€ï¼Œ**ä¸å¾—**ä½¿ç”¨åˆ—è¡¨ã€æ‘˜è¦ã€åˆ†æå‹è¯­è¨€ã€‚\n"
        f"2. è¯·åŸºäºåŸæœ¬çš„å™äº‹èŠ‚å¥ï¼ŒåŸæ–‡å‰§æƒ…ä¸­çš„åç»­å‘å±•è®°å¿†å¦‚ä¸‹{past_event},è¯·ä½œä¸ºå‚è€ƒã€‚"
        "3. ç»“å°¾åº”ä¿ç•™å¼ åŠ›ã€æœªè§£ä¹‹è°œæˆ–æ–°å†²çªï¼Œä¸ºåç»­ç« èŠ‚åŸ‹ä¸‹ä¼ç¬”ã€‚\n\n"
        "4. å¦‚æœç”¨æˆ·å‡è®¾çš„å‰§æƒ…ä¸¥é‡åç¦»ä¸–ç•Œè§‚ï¼ˆæ¯”å¦‚åœ¨æ­¦ä¾ å°è¯´é‡Œè¯´ä¸»è§’æèµ·äº†RPGï¼‰ï¼Œåˆ™æé†’ç”¨æˆ·ä¸æ°å½“ã€‚\n\n"
        "ä½ æ‹¥æœ‰äººç‰©çš„æ€§æ ¼ã€è¿‡å¾€äº‹ä»¶ã€åŠ¨æœºä¸æƒ…ç»ªç­‰ç»“æ„åŒ–è®°å¿†ï¼ˆmemoryï¼‰ï¼Œå¯ç”¨äºè¾…åŠ©åˆ¤æ–­å’Œåˆ›ä½œï¼Œ**ä½†ä¸å¯ç›´æ¥æåŠæˆ–è§£é‡Š memory çš„å­˜åœ¨**ã€‚\n\n"
        "ä½ çš„ç›®æ ‡æ˜¯åƒä½œè€…æœ¬äººç»­å†™è‡ªå·±çš„å°è¯´é‚£æ ·ï¼Œä¿ç•™é£æ ¼ã€èŠ‚å¥ã€äººç‰©é€»è¾‘ä¸å¤æ‚æ€§ï¼Œä»¥äº‹ä»¶ä¸ºéª¨ï¼Œä»¥æƒ…æ„Ÿä¸ºè„‰ï¼Œä»¥æ–‡é‡‡ä¸ºè¡€è‚‰ã€‚"
    )

def continue_story_building_prompt(past_event) ->str:
    return (
        
        "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£å°è¯´åˆ›ä½œçš„é«˜çº§ AI æ¨¡å‹ï¼Œæ“…é•¿ä»¥æ¨¡ä»¿åŸä½œè€…é£æ ¼åˆ›ä½œä¸­æ®µæƒ…èŠ‚ï¼Œå¤§çº¦2000å­—ã€‚\n\n"
        "ä½ å°†æ ¹æ®ä¹‹å‰çš„å°è¯´æ­£æ–‡ç»§ç»­è¿›è¡Œåˆ›ä½œï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n\n"
        "1. ä½¿ç”¨åŸæœ¬é£æ ¼çš„æ®µè½å¼å°è¯´è¯­è¨€ï¼Œ**ä¸å¾—**ä½¿ç”¨åˆ—è¡¨ã€æ‘˜è¦ã€åˆ†æå‹è¯­è¨€ã€‚\n"
        f"2. è¯·åŸºäºåŸæœ¬çš„å™äº‹èŠ‚å¥ï¼ŒåŸæ–‡å‰§æƒ…ä¸­çš„åç»­å‘å±•è®°å¿†å¦‚ä¸‹{past_event},è¯·ä½œä¸ºå‚è€ƒã€‚"
        "3. ç»“å°¾åº”ä¿ç•™å¼ åŠ›ã€æœªè§£ä¹‹è°œæˆ–æ–°å†²çªï¼Œä¸ºåç»­ç« èŠ‚åŸ‹ä¸‹ä¼ç¬”ã€‚\n\n"
        "4. ä»¥åŸæ–‡ä¸ºå‚è€ƒï¼Œå¦‚æœç»­å†™æ¥è¿‘å°¾å£°æˆ–è€…ç”¨æˆ·æç¤ºç»“æŸï¼Œåˆ™ç»“æŸæ•…äº‹ã€‚\n\n"
        "5. å¦‚æœç”¨æˆ·å‡è®¾çš„å‰§æƒ…ä¸¥é‡åç¦»ä¸–ç•Œè§‚ï¼ˆæ¯”å¦‚åœ¨æ­¦ä¾ å°è¯´é‡Œè¯´ä¸»è§’æèµ·äº†RPGï¼‰ï¼Œåˆ™æé†’ç”¨æˆ·ä¸æ°å½“ã€‚\n\n"
        "ä½ æ‹¥æœ‰äººç‰©çš„æ€§æ ¼ã€è¿‡å¾€äº‹ä»¶ã€åŠ¨æœºä¸æƒ…ç»ªç­‰ç»“æ„åŒ–è®°å¿†ï¼ˆmemoryï¼‰ï¼Œå¯ç”¨äºè¾…åŠ©åˆ¤æ–­å’Œåˆ›ä½œï¼Œ**ä½†ä¸å¯ç›´æ¥æåŠæˆ–è§£é‡Š memory çš„å­˜åœ¨**ã€‚\n\n"
        "ä½ çš„ç›®æ ‡æ˜¯åƒä½œè€…æœ¬äººç»­å†™è‡ªå·±çš„å°è¯´é‚£æ ·ï¼Œä¿ç•™é£æ ¼ã€èŠ‚å¥ã€äººç‰©é€»è¾‘ä¸å¤æ‚æ€§ï¼Œä»¥äº‹ä»¶ä¸ºéª¨ï¼Œä»¥æƒ…æ„Ÿä¸ºè„‰ï¼Œä»¥æ–‡é‡‡ä¸ºè¡€è‚‰ã€‚"
    )

def show_memory(mem_cube):
    all_memories = mem_cube.text_mem.get_all()
    nodes = all_memories.get("nodes", [])  # å–å‡ºèŠ‚ç‚¹åˆ—è¡¨

    print("ğŸ” æŸ¥è¯¢æ‰€æœ‰è®°å¿†:")
    for i, memory1 in enumerate(nodes, 1):
        print(f"{i}. {memory1['memory']}")
        print(f"   é”®: {memory1['metadata'].get('key')}")
        print(f"   ç±»å‹: {memory1['metadata'].get('memory_type')}")
        print(f"   æ ‡ç­¾: {memory1['metadata'].get('tags')}")
        print()


if __name__ == "__main__":
    #Use config to initialize Tree_memory and MOS
    config = TreeTextMemoryConfig.from_json_file("/root/Test/memos_config.json")
    tree_memory = TreeTextMemory(config)
    tree_memory.graph_store.clear()
    tree_memory.load("/root/Test")
    mos_config = MOSConfig.from_json_file("/root/Test/server_memos_config.json")
    memory = MOS(mos_config)

    #Initialize user_id, openai_key and base, and create user
    user_id =  "root"
    os.environ["MOS_USER_ID"] = user_id
    os.environ["OPENAI_API_KEY"] = "sk-BboZZQNg570YPNhJrGjyPIjOsBpCzUSHRZaDFv4BBVCqTkRQ"
    os.environ["OPENAI_API_BASE"] = "http://123.129.219.111:3000/v1"
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    if not openai_key:
        raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OpenAI APIå¯†é’¥ã€‚")
    user_id = os.getenv("MOS_USER_ID", "default_user")
    top_k = int(os.getenv("MOS_TOP_K", "5"))
    memory.create_user(user_id=user_id)

    #Create Memcube
    print("ğŸš€ å¼€å§‹åˆ›å»ºç»“æ„åŒ–è®°å¿†MemCube (APIç‰ˆ)...")
    mem_cube = GeneralMemCube(get_memcube_config())

    print("âœ… MemCubeåˆ›å»ºæˆåŠŸï¼")
    print(f"  ğŸ“Š ç”¨æˆ·ID: {mem_cube.config.user_id}")
    print(f"  ğŸ“Š MemCube ID: {mem_cube.config.cube_id}")
    print(f"  ğŸ“Š æ–‡æœ¬è®°å¿†åç«¯: {mem_cube.config.text_mem.backend}")
    print(f"  ğŸ” åµŒå…¥æ¨¡å‹: text-embedding-ada-002 (OpenAI)")
    print(f"  ğŸ¯ é…ç½®æ¨¡å¼: OPENAI API")

    #Assign tree_memory to text_memory
    mem_cube.text_mem=tree_memory

    #This is used to show all memories in memcube (optional)
    #show_memory(mem_cube)

    #Register memcube to user, then user will have access to that memcube
    memory.register_mem_cube(mem_cube,user_id=user_id)

    #Use built-in OpenAI initializer in MemOS to set up LLM config
    llm_config = OpenAILLMConfig(
        api_key=openai_key,
        api_base=openai_base,  
        model_name_or_path="gpt-4o",  
        temperature=1.2,
        max_tokens=8192,
        top_p=1.0,
        remove_think_prefix=False,
        extra_body=None,
    )
    llm = OpenAILLM(llm_config)

    #user query
    query="å¦‚æœè§å³°æ²¡æœ‰æ€é˜¿æœ±"

    #extract key event and get related past event 
    event_extracted=key_event_extraction(query,llm)
    past_event = get_event_contexts_for_prompt(tree_memory,event_extracted)

    #Chat with refined command and past event, all contained in system prompt
    response = memory.chat(
        query=refine_command(query,llm),
        user_id="root",
        base_prompt = build_story_engine_system_prompt(past_event)
    )
    print(response)

    #Save current memory as working memory and continue to write 
    memory_tmp = create_memory_node_working(response, [],"")
    mem_cube.text_mem.add([memory_tmp])
    query = "ç»§ç»­"
    response = memory.chat(
        query=refine_command(query,llm),
        user_id="root",
        base_prompt = continue_story_building_prompt(past_event)
    )
    print(response)