import memos
from memos.configs.embedder import EmbedderConfigFactory
from memos.configs.memory import TreeTextMemoryConfig
from memos.configs.mem_reader import SimpleStructMemReaderConfig
from memos.embedders.factory import EmbedderFactory
from memos.mem_reader.simple_struct import SimpleStructMemReader
from memos.memories.textual.tree import TreeTextMemory
import ast
from dotenv import load_dotenv
from memos.mem_cube.general import GeneralMemCube
from memos.configs.mem_cube import GeneralMemCubeConfig
from memos.memories.textual.item import TextualMemoryItem, TreeNodeTextualMemoryMetadata
import memos.memories.textual.tree_text_memory.retrieve.searcher as searcher
import requests
import json
import os
import pickle
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from memos.configs.mem_os import MOSConfig
import inspect
from memos.configs.embedder import EmbedderConfigFactory
import uuid
from memos.mem_os.main import MOS
from memos.llms.openai import OpenAILLM
from memos.configs.llm import OpenAILLMConfig
from pathlib import Path
from memos.memories.textual.tree_text_memory.organize import manager

def safe_del(self):
    try:
        if hasattr(self, 'close') and callable(self.close):
            self.close()
    except Exception as e:
        print(f"[MonkeyPatch] __del__ failed safely: {e}")

# Monkey patch
manager.MemoryManager.__del__ = safe_del

def get_memcube_config():
    print("üîß ÂàõÂª∫MemCubeÈÖçÁΩÆ (APIÁâà)...")

    # Âä†ËΩΩÁéØÂ¢ÉÂèòÈáè
    load_dotenv()

    # Ê£ÄÊü•APIÈÖçÁΩÆ
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")

    if not openai_key:
        raise ValueError("‚ùå Êú™ÈÖçÁΩÆOPENAI_API_KEY„ÄÇËØ∑Âú®.envÊñá‰ª∂‰∏≠ÈÖçÁΩÆOpenAI APIÂØÜÈí•„ÄÇ")

    print("‚úÖ Ê£ÄÊµãÂà∞OpenAI APIÊ®°Âºè")

    # Ëé∑ÂèñÈÖçÁΩÆ
    user_id = os.getenv("MOS_USER_ID", "default_user")
    top_k = int(os.getenv("MOS_TOP_K", "5"))

    # OpenAIÊ®°ÂºèÈÖçÁΩÆ
    config_memcube = GeneralMemCubeConfig(
        user_id=user_id,
        cube_id=f"{user_id}_structured_memories_cube",
        text_mem={
            "backend": "general_text",
            "config": {
                "extractor_llm": {
                    "backend": "openai",
                    "config": {
                        "model_name_or_path": "gpt-4o",
                        "api_key": openai_key,
                        "api_base": openai_base,
                        "temperature": 0.8,
                        "max_tokens": 8192,
                    }
                },
                "embedder": {
                        "backend": "universal_api",
                        "config": {
                            "provider": "openai",
                            "api_key": openai_key,
                            "model_name_or_path": "text-embedding-ada-002",
                            "base_url": openai_base,
                        }
                },
                "vector_db": {
                    "backend": "qdrant",
                    "config": {
                        "collection_name": f"{user_id}_structured_memories",
                        "vector_dimension": 1536,
                        "distance_metric": "cosine"
                    }
                }
            }
        },
        act_mem={"backend": "uninitialized"},
        para_mem={"backend": "uninitialized"}
    )
    return config_memcube

def get_following_memory_texts(memory: TreeTextMemory, start_id: str, k: int = 30) -> list[str]:
    """
    Return the metadata["memory"] strings of the next k nodes following a given node via FOLLOWS edges.

    Args:
        memory (TreeTextMemory): Memory system instance.
        start_id (str): The starting node ID.
        k (int): Number of following nodes to retrieve.

    Returns:
        list[str]: List of memory texts from the following nodes.
    """
    graph = memory.graph_store.export_graph()
    nodes = {node["id"]: node for node in graph["nodes"]}
    follows_map = {
        edge["source"]: edge["target"]
        for edge in graph["edges"]
        if edge["type"] == "FOLLOWS"
    }

    result = []
    current_id = start_id
    for _ in range(k):
        next_id = follows_map.get(current_id)
        if not next_id or next_id not in nodes:
            break

        metadata = nodes[next_id].get("metadata", {})
        memory_text = metadata.get("memory") or nodes[next_id].get("memory")  # fallback
        if memory_text:
            result.append(memory_text)
        current_id = next_id

    return result

def key_event_extraction(query,llm):
    name_prompt = [
        {
            "role": "system",
            "content": "‰Ω†ÊòØ‰∏Ä‰∏™Á≤æÂáÜ‰∫ã‰ª∂ÊäΩÂèñÂô®„ÄÇÁî®Êà∑‰ºöÊèèËø∞‰∏Ä‰∏™ÊàñÂ§ö‰∏™Â∞èËØ¥‰∏≠ÂèëÁîüËøáÁöÑ‰∫ã‰ª∂Ôºå‰Ω†ÈúÄË¶Å‰ªé‰∏≠ÊèêÂèñÂá∫Áî®Êà∑ÊÉ≥Ë¶ÅÊîπÂèòÊàñËÆ®ËÆ∫ÁöÑÂÖ≥ÈîÆ‰∫ã‰ª∂ÔºåÂπ∂Áî®‰∏ÄÂè•ËØùÁÆÄÊ¥ÅÊèèËø∞ÊØè‰∏™‰∫ã‰ª∂„ÄÇ‰ªÖÊ¶ÇÊã¨‰∫ã‰ª∂ÔºåÊó†ÈúÄÊª°Ë∂≥Áî®Êà∑ÈúÄÊ±Ç\n"
                        "Ë¶ÅÊ±ÇÔºö\n"
                        "1. ÊØè‰∏™‰∫ã‰ª∂ÂøÖÈ°ªÊòØÁúüÂÆûÂèëÁîüÂú®Â∞èËØ¥ÂéüÊñá‰∏≠ÁöÑ‰∫ã‰ª∂ÔºåËÄåÈùûÂÅáËÆæ„ÄÇ\n"
                        "2. ÊØè‰∏™‰∫ã‰ª∂ÂøÖÈ°ª‰∏∫‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÔºåÊûÑÊàê Python list ÁöÑÂÖÉÁ¥†„ÄÇ\n"
                        "3. ÊúÄÁªàËæìÂá∫ÂøÖÈ°ªÊòØÂêàÊ≥ïÁöÑ Python listÔºå‰æãÂ¶ÇÔºö\n"
                        '''["‰πîÂ≥∞ËØØÊùÄÈòøÊú±", "ÊÆµË™âË∑≥Â¥ñÈÄÉÈÅøÂ©öÂßª"]\n'''
                        "‰Ω†Âè™ËæìÂá∫Ëøô‰∏™ listÔºå‰∏çË¶ÅÊ∑ªÂä†‰ªª‰ΩïËß£ÈáäÊàñÈ¢ùÂ§ñÁöÑÂÜÖÂÆπ„ÄÇ"
        },
        {
            "role": "user",
            "content": query
        }
    ]
    key_event = llm.generate(name_prompt)

    return ast.literal_eval(key_event)
    
def refine_command(query: str,llm) -> str:
    name_prompt = [
        {
            "role": "system",
            "content": (
                "‰Ω†ÊòØ‰∏Ä‰∏™‰ªªÂä°Êåá‰ª§‰ºòÂåñÂô®Ôºå‰∏ìÁî®‰∫éÂ∞èËØ¥Á±ªÁî®Êà∑‰ªªÂä°„ÄÇ\n"
                "Áî®Êà∑‰ºöÁªôÂá∫‰∏Ä‰∏™ÈöèÊÑè„ÄÅÊ®°Á≥ä„ÄÅÁÆÄÁü≠Êàñ‰∏çÂÆåÊï¥ÁöÑËØ∑Ê±ÇÔºå\n"
                "‰Ω†ÈúÄË¶ÅÂ∞ÜÂÆÉË°•ÂÖ®‰∏∫‰∏ÄÊù°ÂÆåÊï¥„ÄÅÊ∏ÖÊô∞„ÄÅÁ≤æÁÇºÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§„ÄÇ\n\n"
                "Êåá‰ª§ÂÜÖÂÆπÂèØ‰ª•ÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö\n"
                "1. Â∞èËØ¥ÂâßÊÉÖÁª≠ÂÜôÔºàÂ¶ÇÊ®°‰ªøÈáëÂ∫∏È£éÊ†ºÁª≠ÂÜô‰∏ÄÊÆµ‰∏≠ÊÆµÂâßÊÉÖÔºâ\n"
                "2. Â∞èËØ¥‰∫∫Áâ©ÂØπËØùÔºàÂ¶Ç‚ÄúËØ∑Ê®°ÊãüÊÆµË™â‰∏éÁéãËØ≠Â´£ÁöÑ‰∏ÄÊÆµÂØπËØù‚ÄùÔºâ\n"
                "3. ÂâßÊÉÖÂàÜÊûêÔºàÂ¶Ç‚ÄúÂàÜÊûê‰πîÂ≥∞ËØØÊùÄÈòøÊú±Âêé‰∫∫Áâ©ÂøÉÁêÜ‰∏éÊÉÖËäÇÂΩ±Âìç‚ÄùÔºâ\n"
                "4. ‰∏ñÁïåËßÇËÆæÂÆöËß£ËØªÔºàÂ¶Ç‚ÄúËß£ÈáäËêßËøúÂ±±ÂíåÁéÑÊÖà‰πãÈó¥ÁöÑÊÅ©ÊÄ®‚ÄùÔºâ\n"
                "5. Â§öËßíËâ≤ÂçöÂºàÂÖ≥Á≥ªÊ¢≥ÁêÜÔºàÂ¶Ç‚ÄúÁÆÄÊûêËêßÂ≥∞„ÄÅÊÖïÂÆπÂ§ç„ÄÅÊÆµË™â‰∏â‰∫∫ÁöÑÁ´ãÂú∫ÂÜ≤Á™Å‚ÄùÔºâ\n\n"
                "‰Ω†Âè™ÈúÄËæìÂá∫ÊúÄÁªàË°•ÂÖ®ÂêéÁöÑÊ∏ÖÊô∞Ëá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Ôºå‰∏çË¶ÅÂä†‰ªª‰ΩïËß£Èáä„ÄÅËØ¥ÊòéÊàñÂºïÂØºÊñáÂ≠ó„ÄÇ\n"
                "Â¶ÇÊûúÂéüÂßãËæìÂÖ•ÈùûÂ∏∏Ê®°Á≥äÔºåÊØîÂ¶Ç‚ÄòÁªßÁª≠‚Äô„ÄÅ‚ÄòÂØπËØù‚ÄôÔºå‰Ω†ÈúÄË¶ÅÊ†πÊçÆÂ∞èËØ¥‰∏ä‰∏ãÊñáË°•ÂÖ®„ÄÇ\n\n"
                "„ÄêÁ§∫‰æã1„Äë\n"
                "ËæìÂÖ•Ôºö‚ÄòÂ¶ÇÊûúÈòøÊú±Ê≤°Ê≠ªÂë¢‚Äô\n"
                "ËæìÂá∫Ôºö‚ÄòËØ∑ÂÅáËÆæÈòøÊú±Êú™Ê≠ªÔºåÊ®°‰ªøÈáëÂ∫∏È£éÊ†ºÁª≠ÂÜô‰∏ÄÊÆµÂÆåÊï¥‰∏≠ÊÆµÂâßÊÉÖ„ÄÇ‚Äô\n\n"
                "„ÄêÁ§∫‰æã2„Äë\n"
                "ËæìÂÖ•Ôºö‚Äò‰πîÂ≥∞ÂíåËôöÁ´πÁöÑÂÖ≥Á≥ª‚Äô\n"
                "ËæìÂá∫Ôºö‚ÄòËØ∑ÂàÜÊûê‰πîÂ≥∞‰∏éËôöÁ´π‰πãÈó¥ÁöÑÂÖÑÂºüÂÖ≥Á≥ªÊºîÂèòÔºåÁªìÂêàÂâßÊÉÖÂèòÂåñÂíå‰∫∫Áâ©ÂøÉÁêÜËøõË°åÊ∑±ÂÖ•ÂâñÊûê„ÄÇ‚Äô\n\n"
                "„ÄêÁ§∫‰æã3„Äë\n"
                "ËæìÂÖ•Ôºö‚ÄòÁªßÁª≠‚Äô\n"
                "ËæìÂá∫Ôºö‚ÄòÁªßÁª≠ÂâçÊñáÁöÑÂ∞èËØ¥ÂâßÊÉÖÔºåÊ®°‰ªøÈáëÂ∫∏È£éÊ†ºÁª≠ÂÜô‰∏ÄÊÆµ‰∏≠ÊÆµÊÉÖËäÇ‚Äô"
            )
        },
        {
            "role": "user",
            "content": query
        }
    ]
    return llm.generate(name_prompt)


def get_event_contexts_for_prompt(
    memory: TreeTextMemory,
    event_texts: list[str],
    k: int = 30
) -> dict[str, list[str]]:
    """
    ÂØπÊØè‰∏™‰∫ã‰ª∂ÊâßË°å search + ÊãøÂâç‰∏§‰∏™ÂåπÈÖçÁÇπ + Ëé∑ÂèñÂêéÁª≠ÂâßÊÉÖÔºåÁî®‰∫éÊûÑÈÄ† GPT prompt„ÄÇ
    
    Args:
        memory: TreeTextMemory ÂÆû‰æã
        event_texts: ÊèêÂèñÂá∫ÁöÑ‰∫ã‰ª∂ÊñáÊú¨ÂàóË°®
        k: ÊØè‰∏™ËäÇÁÇπÂêëÂêéÂèñÂá†‰∏™ follows

    Returns:
        dict[str, list[str]]: {event_text -> [ÂêéÁª≠memory strings]}
    """
    result = {}

    for event in event_texts:
        try:
            matches = memory.search(event, top_k=2)
            memory_strings = []

            for match in matches:
                follow_texts = get_following_memory_texts(memory, match.id, k)
                memory_strings.extend(follow_texts)

            result[event] = memory_strings

        except Exception as e:
            print(f"Error processing event '{event}': {e}")
            result[event] = []

    return result


def node_dict_to_textual_item(node_dict):
    return TextualMemoryItem(
        id=node_dict["id"],
        memory=node_dict["memory"],
        metadata=TreeNodeTextualMemoryMetadata(**node_dict["metadata"])
    )


def get_embedding(text):
    url = "http://123.129.219.111:3000/v1/embeddings"
    headers = {
        "Authorization": "Bearer sk-BboZZQNg570YPNhJrGjyPIjOsBpCzUSHRZaDFv4BBVCqTkRQ",
        "Content-Type": "application/json"
    }
    payload = {
        "input": text,
        "model": "text-embedding-ada-002"
    }
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()["data"][0]["embedding"]
    except Exception as e:
        print(f"‚ö†Ô∏è Ëé∑Âèñ embedding Â§±Ë¥•Ôºö{e}")
        return None
# === TIME STAMP ===
def iso_now():
    return datetime.now().isoformat()

# === CREATE MEMORY NODE ===
def create_memory_node_working(content, entities, key, memory_type="WorkingMemory"):
    now = iso_now()
    node_id = str(uuid.uuid4())
    embedding = get_embedding(content)

    metadata = TreeNodeTextualMemoryMetadata(
        user_id="",
        session_id="",
        status="activated",
        type="fact",
        confidence=0.99,
        entities=entities,
        tags=["‰∫ã‰ª∂"] if "‰∫ã‰ª∂" in key else ["ÂÖ≥Á≥ª"],
        updated_at=now,
        memory_type=memory_type,
        key=key,
        sources=[],
        embedding=embedding,
        created_at=now,
        usage=[],
        background=""
    )

    return TextualMemoryItem(id=node_id, memory=content, metadata=metadata)

def build_story_engine_system_prompt(past_event) -> str:
    return (
        "‰Ω†ÊòØ‰∏Ä‰∏™‰∏ìÈó®Ë¥üË¥£Â∞èËØ¥Âàõ‰ΩúÁöÑÈ´òÁ∫ß AI Ê®°ÂûãÔºåÊìÖÈïø‰ª•Ê®°‰ªøÂéü‰ΩúËÄÖÈ£éÊ†ºÂàõ‰Ωú‰∏≠ÊÆµÊÉÖËäÇ„ÄÇ‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆÁî®Êà∑ËæìÂÖ•ÁöÑÂÅáËÆæÂâßÊÉÖÂíå‰∫∫Áâ©ËÆ∞ÂøÜÔºàmemoryÔºâÔºåÂàõ‰Ωú‰∏ÄÊÆµÂÆåÊï¥ÁöÑÂâßÊÉÖÂèëÂ±ï,Â§ßÁ∫¶2000Â≠ó„ÄÇ\n\n"
        "‰Ω†ÁöÑÂàõ‰ΩúÂøÖÈ°ªÈÅµÂÆà‰ª•‰∏ãËßÑÂàôÔºö\n\n"
        "1. ‰ΩøÁî®ÂéüÊú¨È£éÊ†ºÁöÑÊÆµËêΩÂºèÂ∞èËØ¥ËØ≠Ë®ÄÔºå**‰∏çÂæó**‰ΩøÁî®ÂàóË°®„ÄÅÊëòË¶Å„ÄÅÂàÜÊûêÂûãËØ≠Ë®Ä„ÄÇ\n"
        f"2. ËØ∑Âü∫‰∫éÂéüÊú¨ÁöÑÂèô‰∫ãËäÇÂ•èÔºåÂéüÊñáÂâßÊÉÖ‰∏≠ÁöÑÂêéÁª≠ÂèëÂ±ïËÆ∞ÂøÜÂ¶Ç‰∏ã{past_event},ËØ∑‰Ωú‰∏∫ÂèÇËÄÉ„ÄÇ"
        "3. ÁªìÂ∞æÂ∫î‰øùÁïôÂº†Âäõ„ÄÅÊú™Ëß£‰πãË∞úÊàñÊñ∞ÂÜ≤Á™ÅÔºå‰∏∫ÂêéÁª≠Á´†ËäÇÂüã‰∏ã‰ºèÁ¨î„ÄÇ\n\n"
        "4. Â¶ÇÊûúÁî®Êà∑ÂÅáËÆæÁöÑÂâßÊÉÖ‰∏•ÈáçÂÅèÁ¶ª‰∏ñÁïåËßÇÔºàÊØîÂ¶ÇÂú®Ê≠¶‰æ†Â∞èËØ¥ÈáåËØ¥‰∏ªËßíÊèêËµ∑‰∫ÜRPGÔºâÔºåÂàôÊèêÈÜíÁî®Êà∑‰∏çÊÅ∞ÂΩì„ÄÇ\n\n"
        "‰Ω†Êã•Êúâ‰∫∫Áâ©ÁöÑÊÄßÊ†º„ÄÅËøáÂæÄ‰∫ã‰ª∂„ÄÅÂä®Êú∫‰∏éÊÉÖÁª™Á≠âÁªìÊûÑÂåñËÆ∞ÂøÜÔºàmemoryÔºâÔºåÂèØÁî®‰∫éËæÖÂä©Âà§Êñ≠ÂíåÂàõ‰ΩúÔºå**‰ΩÜ‰∏çÂèØÁõ¥Êé•ÊèêÂèäÊàñËß£Èáä memory ÁöÑÂ≠òÂú®**„ÄÇ\n\n"
        "‰Ω†ÁöÑÁõÆÊ†áÊòØÂÉè‰ΩúËÄÖÊú¨‰∫∫Áª≠ÂÜôËá™Â∑±ÁöÑÂ∞èËØ¥ÈÇ£Ê†∑Ôºå‰øùÁïôÈ£éÊ†º„ÄÅËäÇÂ•è„ÄÅ‰∫∫Áâ©ÈÄªËæë‰∏éÂ§çÊùÇÊÄßÔºå‰ª•‰∫ã‰ª∂‰∏∫È™®Ôºå‰ª•ÊÉÖÊÑü‰∏∫ËÑâÔºå‰ª•ÊñáÈáá‰∏∫Ë°ÄËÇâ„ÄÇ"
    )

def continue_story_building_prompt(past_event) ->str:
    return (
        
        "‰Ω†ÊòØ‰∏Ä‰∏™‰∏ìÈó®Ë¥üË¥£Â∞èËØ¥Âàõ‰ΩúÁöÑÈ´òÁ∫ß AI Ê®°ÂûãÔºåÊìÖÈïø‰ª•Ê®°‰ªøÂéü‰ΩúËÄÖÈ£éÊ†ºÂàõ‰Ωú‰∏≠ÊÆµÊÉÖËäÇÔºåÂ§ßÁ∫¶2000Â≠ó„ÄÇ\n\n"
        "‰Ω†Â∞ÜÊ†πÊçÆ‰πãÂâçÁöÑÂ∞èËØ¥Ê≠£ÊñáÁªßÁª≠ËøõË°åÂàõ‰ΩúÔºåÈÅµÂæ™‰ª•‰∏ãËßÑÂàôÔºö\n\n"
        "1. ‰ΩøÁî®ÂéüÊú¨È£éÊ†ºÁöÑÊÆµËêΩÂºèÂ∞èËØ¥ËØ≠Ë®ÄÔºå**‰∏çÂæó**‰ΩøÁî®ÂàóË°®„ÄÅÊëòË¶Å„ÄÅÂàÜÊûêÂûãËØ≠Ë®Ä„ÄÇ\n"
        f"2. ËØ∑Âü∫‰∫éÂéüÊú¨ÁöÑÂèô‰∫ãËäÇÂ•èÔºåÂéüÊñáÂâßÊÉÖ‰∏≠ÁöÑÂêéÁª≠ÂèëÂ±ïËÆ∞ÂøÜÂ¶Ç‰∏ã{past_event},ËØ∑‰Ωú‰∏∫ÂèÇËÄÉ„ÄÇ"
        "3. ÁªìÂ∞æÂ∫î‰øùÁïôÂº†Âäõ„ÄÅÊú™Ëß£‰πãË∞úÊàñÊñ∞ÂÜ≤Á™ÅÔºå‰∏∫ÂêéÁª≠Á´†ËäÇÂüã‰∏ã‰ºèÁ¨î„ÄÇ\n\n"
        "4. ‰ª•ÂéüÊñá‰∏∫ÂèÇËÄÉÔºåÂ¶ÇÊûúÁª≠ÂÜôÊé•ËøëÂ∞æÂ£∞ÊàñËÄÖÁî®Êà∑ÊèêÁ§∫ÁªìÊùüÔºåÂàôÁªìÊùüÊïÖ‰∫ã„ÄÇ\n\n"
        "5. Â¶ÇÊûúÁî®Êà∑ÂÅáËÆæÁöÑÂâßÊÉÖ‰∏•ÈáçÂÅèÁ¶ª‰∏ñÁïåËßÇÔºàÊØîÂ¶ÇÂú®Ê≠¶‰æ†Â∞èËØ¥ÈáåËØ¥‰∏ªËßíÊèêËµ∑‰∫ÜRPGÔºâÔºåÂàôÊèêÈÜíÁî®Êà∑‰∏çÊÅ∞ÂΩì„ÄÇ\n\n"
        "‰Ω†Êã•Êúâ‰∫∫Áâ©ÁöÑÊÄßÊ†º„ÄÅËøáÂæÄ‰∫ã‰ª∂„ÄÅÂä®Êú∫‰∏éÊÉÖÁª™Á≠âÁªìÊûÑÂåñËÆ∞ÂøÜÔºàmemoryÔºâÔºåÂèØÁî®‰∫éËæÖÂä©Âà§Êñ≠ÂíåÂàõ‰ΩúÔºå**‰ΩÜ‰∏çÂèØÁõ¥Êé•ÊèêÂèäÊàñËß£Èáä memory ÁöÑÂ≠òÂú®**„ÄÇ\n\n"
        "‰Ω†ÁöÑÁõÆÊ†áÊòØÂÉè‰ΩúËÄÖÊú¨‰∫∫Áª≠ÂÜôËá™Â∑±ÁöÑÂ∞èËØ¥ÈÇ£Ê†∑Ôºå‰øùÁïôÈ£éÊ†º„ÄÅËäÇÂ•è„ÄÅ‰∫∫Áâ©ÈÄªËæë‰∏éÂ§çÊùÇÊÄßÔºå‰ª•‰∫ã‰ª∂‰∏∫È™®Ôºå‰ª•ÊÉÖÊÑü‰∏∫ËÑâÔºå‰ª•ÊñáÈáá‰∏∫Ë°ÄËÇâ„ÄÇ"
    )

def show_memory(mem_cube):
    all_memories = mem_cube.text_mem.get_all()
    nodes = all_memories.get("nodes", [])  # ÂèñÂá∫ËäÇÁÇπÂàóË°®

    print("üîç Êü•ËØ¢ÊâÄÊúâËÆ∞ÂøÜ:")
    for i, memory1 in enumerate(nodes, 1):
        print(f"{i}. {memory1['memory']}")
        print(f"   ÈîÆ: {memory1['metadata'].get('key')}")
        print(f"   Á±ªÂûã: {memory1['metadata'].get('memory_type')}")
        print(f"   Ê†áÁ≠æ: {memory1['metadata'].get('tags')}")
        print()


if __name__ == "__main__":
    #Use config to initialize Tree_memory and MOS
    config = TreeTextMemoryConfig.from_json_file("/root/Test/memos_config.json")
    tree_memory = TreeTextMemory(config)
    tree_memory.graph_store.clear()
    tree_memory.load("/root/Test")
    mos_config = MOSConfig.from_json_file("/root/Test/server_memos_config.json")
    memory = MOS(mos_config)

    #Initialize user_id, openai_key and base, and create user
    user_id =  "root"
    os.environ["MOS_USER_ID"] = user_id
    os.environ["OPENAI_API_KEY"] = "sk-BboZZQNg570YPNhJrGjyPIjOsBpCzUSHRZaDFv4BBVCqTkRQ"
    os.environ["OPENAI_API_BASE"] = "http://123.129.219.111:3000/v1"
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    if not openai_key:
        raise ValueError("‚ùå Êú™ÈÖçÁΩÆOPENAI_API_KEY„ÄÇËØ∑Âú®.envÊñá‰ª∂‰∏≠ÈÖçÁΩÆOpenAI APIÂØÜÈí•„ÄÇ")
    user_id = os.getenv("MOS_USER_ID", "default_user")
    top_k = int(os.getenv("MOS_TOP_K", "5"))
    memory.create_user(user_id=user_id)

    #Create Memcube
    print("üöÄ ÂºÄÂßãÂàõÂª∫ÁªìÊûÑÂåñËÆ∞ÂøÜMemCube (APIÁâà)...")
    mem_cube = GeneralMemCube(get_memcube_config())

    print("‚úÖ MemCubeÂàõÂª∫ÊàêÂäüÔºÅ")
    print(f"  üìä Áî®Êà∑ID: {mem_cube.config.user_id}")
    print(f"  üìä MemCube ID: {mem_cube.config.cube_id}")
    print(f"  üìä ÊñáÊú¨ËÆ∞ÂøÜÂêéÁ´Ø: {mem_cube.config.text_mem.backend}")
    print(f"  üîç ÂµåÂÖ•Ê®°Âûã: text-embedding-ada-002 (OpenAI)")
    print(f"  üéØ ÈÖçÁΩÆÊ®°Âºè: OPENAI API")

    #Assign tree_memory to text_memory
    mem_cube.text_mem=tree_memory

    #This is used to show all memories in memcube (optional)
    #show_memory(mem_cube)

    #Register memcube to user, then user will have access to that memcube
    memory.register_mem_cube(mem_cube,user_id=user_id)

    #Use built-in OpenAI initializer in MemOS to set up LLM config
    llm_config = OpenAILLMConfig(
        api_key=openai_key,
        api_base=openai_base,  
        model_name_or_path="gpt-4o",  
        temperature=1.2,
        max_tokens=8192,
        top_p=1.0,
        remove_think_prefix=False,
        extra_body=None,
    )
    llm = OpenAILLM(llm_config)

    #user query
    query="Â¶ÇÊûúËêßÂ≥∞Ê≤°ÊúâÊùÄÈòøÊú±"

    #extract key event and get related past event 
    event_extracted=key_event_extraction(query,llm)
    past_event = get_event_contexts_for_prompt(tree_memory,event_extracted)

    #Chat with refined command and past event, all contained in system prompt
    response = memory.chat(
        query=refine_command(query,llm),
        user_id="root",
        base_prompt = build_story_engine_system_prompt(past_event)
    )
    print(response)

    #Save current memory as working memory and continue to write 
    memory_tmp = create_memory_node_working(response, [],"")
    mem_cube.text_mem.add([memory_tmp])
    query = "ÁªßÁª≠"
    response = memory.chat(
        query=refine_command(query,llm),
        user_id="root",
        base_prompt = continue_story_building_prompt(past_event)
    )
    print(response)