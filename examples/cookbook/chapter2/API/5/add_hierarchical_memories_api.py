# add_hierarchical_memories_api.py
# ğŸ¯ æ·»åŠ å±‚æ¬¡åŒ–è®°å¿†çš„ç¤ºä¾‹ (APIç‰ˆ)
import os

from dotenv import load_dotenv

from memos.configs.memory import TreeTextMemoryConfig
from memos.memories.textual.item import TreeNodeTextualMemoryMetadata
from memos.memories.textual.tree import TreeTextMemory


def add_hierarchical_memories_api():
    """
    ğŸ¯ æ·»åŠ å±‚æ¬¡åŒ–è®°å¿†çš„ç¤ºä¾‹ (APIç‰ˆ)
    """

    print("ğŸš€ å¼€å§‹æ·»åŠ å±‚æ¬¡åŒ–è®°å¿† (APIç‰ˆ)...")

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # æ£€æŸ¥APIé…ç½®
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")

    if not openai_key:
        raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OpenAI APIå¯†é’¥ã€‚")

    print("âœ… æ£€æµ‹åˆ°OpenAI APIæ¨¡å¼")

    # è·å–ç”¨æˆ·ID
    user_id = os.getenv("MOS_USER_ID", "default_user")

    # åˆ›å»ºTreeTextMemoryé…ç½®
    tree_config = TreeTextMemoryConfig(
        extractor_llm={
            "backend": "openai",
            "config": {
                "model_name_or_path": "gpt-3.5-turbo",
                "api_key": openai_key,
                "api_base": openai_base,
                "temperature": 0.1,
                "max_tokens": 1024,
            },
        },
        dispatcher_llm={
            "backend": "openai",
            "config": {
                "model_name_or_path": "gpt-3.5-turbo",
                "api_key": openai_key,
                "api_base": openai_base,
                "temperature": 0.1,
                "max_tokens": 1024,
            },
        },
        graph_db={
            "backend": "neo4j",
            "config": {
                "uri": "bolt://localhost:7687",
                "user": "neo4j",
                "password": "password",
                "db_name": f"{user_id}_hierarchical_memory",
                "auto_create": True,
                "embedding_dimension": 1536,
            },
        },
        embedder={
            "backend": "universal_api",
            "config": {
                "provider": "openai",
                "api_key": openai_key,
                "model_name_or_path": "text-embedding-ada-002",
                "base_url": openai_base,
            },
        },
    )

    # åˆ›å»ºTreeTextMemoryå®ä¾‹
    tree_memory = TreeTextMemory(tree_config)

    # æ¸…ç©ºç°æœ‰è®°å¿†
    tree_memory.delete_all()

    # åˆ›å»ºå±‚æ¬¡åŒ–è®°å¿†ç»“æ„
    memories = []

    # æ ¹èŠ‚ç‚¹ï¼šé¡¹ç›®æ¦‚è¿°
    root_metadata = TreeNodeTextualMemoryMetadata(
        user_id=user_id,
        type="topic",
        source="file",
        confidence=95.0,
        memory_type="LongTermMemory",
        key="AIé¡¹ç›®_æ ¹èŠ‚ç‚¹",
        entities=["AIé¡¹ç›®", "æ™ºèƒ½å®¢æœ"],
        tags=["é¡¹ç›®", "æ ¹èŠ‚ç‚¹", "é‡è¦"],
    )

    memories.append(
        {"memory": "AIé¡¹ç›®æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¢æœç³»ç»Ÿï¼Œç›®æ ‡æ˜¯æå‡å®¢æˆ·æœåŠ¡æ•ˆç‡", "metadata": root_metadata}
    )

    # å­èŠ‚ç‚¹1ï¼šæŠ€æœ¯æ¶æ„
    tech_metadata = TreeNodeTextualMemoryMetadata(
        user_id=user_id,
        type="fact",
        source="file",
        confidence=90.0,
        memory_type="LongTermMemory",
        key="æŠ€æœ¯æ¶æ„",
        entities=["NLP", "æœºå™¨å­¦ä¹ ", "API"],
        tags=["æŠ€æœ¯", "æ¶æ„", "é‡è¦"],
    )

    memories.append(
        {
            "memory": "é¡¹ç›®ä½¿ç”¨æœ€æ–°çš„NLPæŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡APIæ¥å£æä¾›æœåŠ¡",
            "metadata": tech_metadata,
        }
    )

    # å­èŠ‚ç‚¹2ï¼šå›¢é˜Ÿä¿¡æ¯
    team_metadata = TreeNodeTextualMemoryMetadata(
        user_id=user_id,
        type="fact",
        source="conversation",
        confidence=85.0,
        memory_type="LongTermMemory",
        key="å›¢é˜Ÿä¿¡æ¯",
        entities=["å¼€å‘å›¢é˜Ÿ", "8äºº"],
        tags=["å›¢é˜Ÿ", "äººå‘˜"],
    )

    memories.append(
        {"memory": "å¼€å‘å›¢é˜Ÿæœ‰8ä¸ªäººï¼ŒåŒ…æ‹¬å‰ç«¯ã€åç«¯ã€AIå·¥ç¨‹å¸ˆå’Œäº§å“ç»ç†", "metadata": team_metadata}
    )

    # å­èŠ‚ç‚¹3ï¼šæ—¶é—´è®¡åˆ’
    timeline_metadata = TreeNodeTextualMemoryMetadata(
        user_id=user_id,
        type="procedure",
        source="file",
        confidence=80.0,
        memory_type="WorkingMemory",
        key="æ—¶é—´è®¡åˆ’",
        entities=["6ä¸ªæœˆ", "é‡Œç¨‹ç¢‘"],
        tags=["è®¡åˆ’", "æ—¶é—´", "ä¸´æ—¶"],
    )

    memories.append(
        {
            "memory": "é¡¹ç›®é¢„è®¡6ä¸ªæœˆå®Œæˆï¼Œåˆ†ä¸ºéœ€æ±‚åˆ†æã€è®¾è®¡ã€å¼€å‘ã€æµ‹è¯•å››ä¸ªé˜¶æ®µ",
            "metadata": timeline_metadata,
        }
    )

    # æ·»åŠ è®°å¿†åˆ°å›¾æ•°æ®åº“
    tree_memory.add(memories)

    print("âœ… æˆåŠŸæ·»åŠ äº†4ä¸ªå±‚æ¬¡åŒ–è®°å¿†èŠ‚ç‚¹")

    # æœç´¢è®°å¿†
    print("\nğŸ” æœç´¢åŒ…å«'æŠ€æœ¯'çš„è®°å¿†:")
    search_results = tree_memory.search("æŠ€æœ¯", top_k=3)
    for i, result in enumerate(search_results, 1):
        print(f"{i}. {result.memory}")
        print(f"   é”®: {result.metadata.key}")
        print(f"   ç±»å‹: {result.metadata.memory_type}")
        print(f"   æ ‡ç­¾: {result.metadata.tags}")
        print()

    # è·å–å·¥ä½œè®°å¿†
    print("ğŸ” è·å–å·¥ä½œè®°å¿†:")
    working_memories = tree_memory.get_working_memory()
    for memory in working_memories:
        print(f"- {memory.memory}")

    return tree_memory


if __name__ == "__main__":
    add_hierarchical_memories_api()
