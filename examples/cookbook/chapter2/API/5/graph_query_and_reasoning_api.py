# graph_query_and_reasoning_api.py
# ğŸ¯ å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†ç¤ºä¾‹ (APIç‰ˆ)
import os

from dotenv import load_dotenv

from memos.configs.memory import TreeTextMemoryConfig
from memos.memories.textual.tree import TreeTextMemory


def graph_query_and_reasoning_api():
    """
    ğŸ¯ å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†ç¤ºä¾‹ (APIç‰ˆ)
    """

    print("ğŸš€ å¼€å§‹å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç† (APIç‰ˆ)...")

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # æ£€æŸ¥APIé…ç½®
    openai_key = os.getenv("OPENAI_API_KEY")
    openai_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")

    if not openai_key:
        raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OpenAI APIå¯†é’¥ã€‚")

    print("âœ… æ£€æµ‹åˆ°OpenAI APIæ¨¡å¼")

    # è·å–ç”¨æˆ·ID
    user_id = os.getenv("MOS_USER_ID", "default_user")

    # åˆ›å»ºTreeTextMemoryé…ç½®
    tree_config = TreeTextMemoryConfig(
        extractor_llm={
            "backend": "openai",
            "config": {
                "model_name_or_path": "gpt-3.5-turbo",
                "api_key": openai_key,
                "api_base": openai_base,
                "temperature": 0.1,
                "max_tokens": 1024,
            },
        },
        dispatcher_llm={
            "backend": "openai",
            "config": {
                "model_name_or_path": "gpt-3.5-turbo",
                "api_key": openai_key,
                "api_base": openai_base,
                "temperature": 0.1,
                "max_tokens": 1024,
            },
        },
        graph_db={
            "backend": "neo4j",
            "config": {
                "uri": "bolt://localhost:7687",
                "user": "neo4j",
                "password": "password",
                "db_name": f"{user_id}_reasoning_memory",
                "auto_create": True,
                "embedding_dimension": 1536,
            },
        },
        embedder={
            "backend": "universal_api",
            "config": {
                "provider": "openai",
                "api_key": openai_key,
                "model_name_or_path": "text-embedding-ada-002",
                "base_url": openai_base,
            },
        },
    )

    # åˆ›å»ºTreeTextMemoryå®ä¾‹
    tree_memory = TreeTextMemory(tree_config)

    print("ğŸ” æ‰§è¡Œå›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†...")

    # 1. å‘é‡ç›¸ä¼¼åº¦æœç´¢
    print("\n1ï¸âƒ£ å‘é‡ç›¸ä¼¼åº¦æœç´¢:")
    vector_results = tree_memory.search("AIé¡¹ç›®", top_k=3)
    for i, result in enumerate(vector_results, 1):
        print(f"   {i}. {result.memory}")

    # 2. è·å–æ‰€æœ‰è®°å¿†
    print("\n2ï¸âƒ£ è·å–æ‰€æœ‰è®°å¿†:")
    all_memories = tree_memory.get_all()
    print(f"   æ€»è®°å¿†æ•°é‡: {len(all_memories.get('nodes', []))}")

    # 3. æ›¿æ¢å·¥ä½œè®°å¿†
    print("\n3ï¸âƒ£ æ›¿æ¢å·¥ä½œè®°å¿†:")
    new_working_memories = [
        {
            "memory": "å½“å‰æ­£åœ¨è¿›è¡Œéœ€æ±‚åˆ†æé˜¶æ®µï¼Œéœ€è¦æ”¶é›†ç”¨æˆ·åé¦ˆ",
            "metadata": {
                "memory_type": "WorkingMemory",
                "key": "å½“å‰çŠ¶æ€",
                "tags": ["çŠ¶æ€", "å½“å‰"],
            },
        }
    ]
    tree_memory.replace_working_memory(new_working_memories)
    print("   âœ… å·¥ä½œè®°å¿†å·²æ›´æ–°")

    # 4. å¤‡ä»½è®°å¿†
    print("\n4ï¸âƒ£ å¤‡ä»½è®°å¿†åˆ°æ–‡ä»¶:")
    backup_dir = "tmp/tree_memory_backup"
    tree_memory.dump(backup_dir)
    print(f"   âœ… è®°å¿†å·²å¤‡ä»½åˆ°: {backup_dir}")

    print("\nğŸ¯ é…ç½®æ¨¡å¼: OPENAI API")
    print("ğŸ¤– èŠå¤©æ¨¡å‹: gpt-3.5-turbo (OpenAI)")
    print("ğŸ” åµŒå…¥æ¨¡å‹: text-embedding-ada-002 (OpenAI)")

    return tree_memory


if __name__ == "__main__":
    graph_query_and_reasoning_api()
