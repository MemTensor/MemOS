# memcube_with_structured_memories_ollama.py
# ğŸ¯ å°†ç»“æ„åŒ–è®°å¿†æ·»åŠ åˆ°MemCubeçš„å®Œæ•´ç¤ºä¾‹ (Ollamaç‰ˆ)
import os
from dotenv import load_dotenv
from memos.mem_cube.general import GeneralMemCube
from memos.configs.mem_cube import GeneralMemCubeConfig
from memos.memories.textual.item import TextualMemoryItem, TreeNodeTextualMemoryMetadata

def create_memcube_config_ollama():
    """
    ğŸ¯ åˆ›å»ºMemCubeé…ç½® (Ollamaç‰ˆ)
    """
    
    print("ğŸ”§ åˆ›å»ºMemCubeé…ç½® (Ollamaç‰ˆ)...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥Ollamaé…ç½®
    ollama_base_url = os.getenv("OLLAMA_BASE_URL")
    ollama_chat_model = os.getenv("OLLAMA_CHAT_MODEL")
    ollama_embed_model = os.getenv("OLLAMA_EMBED_MODEL")
    
    if not ollama_base_url or not ollama_chat_model or not ollama_embed_model:
        raise ValueError("âŒ æœªé…ç½®Ollamaç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OLLAMA_BASE_URLã€OLLAMA_CHAT_MODELã€OLLAMA_EMBED_MODELã€‚")
    
    print("âœ… æ£€æµ‹åˆ°Ollamaæœ¬åœ°æ¨¡å‹æ¨¡å¼")
    
    # è·å–é…ç½®
    user_id = os.getenv("MOS_USER_ID", "default_user")
    top_k = int(os.getenv("MOS_TOP_K", "5"))
    
    # Ollamaæ¨¡å¼é…ç½®
    cube_config = {
        "user_id": user_id,
        "cube_id": f"{user_id}_structured_memories_cube",
        "text_mem": {
            "backend": "general_text",
            "config": {
                "extractor_llm": {
                    "backend": "ollama",
                    "config": {
                        "model_name_or_path": ollama_chat_model,
                        "api_base": ollama_base_url
                    }
                },
                "embedder": {
                    "backend": "ollama",
                    "config": {
                        "model_name_or_path": ollama_embed_model,
                        "api_base": ollama_base_url
                    }
                },
                "vector_db": {
                    "backend": "qdrant",
                    "config": {
                        "collection_name": f"{user_id}_structured_memories",
                        "vector_dimension": 768,
                        "distance_metric": "cosine"
                    }
                }
            }
        },
        "act_mem": {"backend": "uninitialized"},
        "para_mem": {"backend": "uninitialized"}
    }
    
    # åˆ›å»ºMemCubeå®ä¾‹
    config_obj = GeneralMemCubeConfig.model_validate(cube_config)
    
    return config_obj

def create_structured_memories_ollama():
    """
    ğŸ¯ å°†ç»“æ„åŒ–è®°å¿†æ·»åŠ åˆ°MemCubeçš„å®Œæ•´ç¤ºä¾‹ (Ollamaç‰ˆ)
    """
    
    print("ğŸš€ å¼€å§‹åˆ›å»ºç»“æ„åŒ–è®°å¿†MemCube (Ollamaç‰ˆ)...")
    
    # åˆ›å»ºMemCubeé…ç½®
    config = create_memcube_config_ollama()
    
    # åˆ›å»ºMemCube
    mem_cube = GeneralMemCube(config)
    
    print("âœ… MemCubeåˆ›å»ºæˆåŠŸï¼")
    print(f"  ğŸ“Š ç”¨æˆ·ID: {mem_cube.config.user_id}")
    print(f"  ğŸ“Š MemCube ID: {mem_cube.config.cube_id}")
    print(f"  ğŸ“Š æ–‡æœ¬è®°å¿†åç«¯: {mem_cube.config.text_mem.backend}")
    
    # è·å–Ollamaé…ç½®ç”¨äºæ˜¾ç¤º
    load_dotenv()
    ollama_embed_model = os.getenv("OLLAMA_EMBED_MODEL")
    ollama_chat_model = os.getenv("OLLAMA_CHAT_MODEL")
    print(f"  ğŸ” åµŒå…¥æ¨¡å‹: {ollama_embed_model} (Ollama)")
    print(f"  ğŸ¤– èŠå¤©æ¨¡å‹: {ollama_chat_model} (Ollama)")
    print(f"  ğŸ¯ é…ç½®æ¨¡å¼: OLLAMA")
    
    # åˆ›å»ºå¤šä¸ªè®°å¿†é¡¹
    memories = []

    # è®°å¿†1ï¼šäººç‰©ä¿¡æ¯
    person_metadata = TreeNodeTextualMemoryMetadata(
        user_id=mem_cube.config.user_id,
        type="fact",
        source="conversation",
        confidence=90.0,
        memory_type="LongTermMemory",
        key="æå››_ä¿¡æ¯",
        entities=["æå››", "è®¾è®¡å¸ˆ"],
        tags=["äººå‘˜", "è®¾è®¡"]
    )

    memories.append({
        "memory": "æå››æ˜¯æˆ‘ä»¬çš„UIè®¾è®¡å¸ˆï¼Œæœ‰5å¹´ç»éªŒï¼Œæ“…é•¿ç”¨æˆ·ç•Œé¢è®¾è®¡",
        "metadata": person_metadata
    })

    # è®°å¿†2ï¼šé¡¹ç›®ä¿¡æ¯
    project_metadata = TreeNodeTextualMemoryMetadata(
        user_id=mem_cube.config.user_id,
        type="fact",
        source="file",
        confidence=95.0,
        memory_type="LongTermMemory",
        key="ç§»åŠ¨åº”ç”¨é¡¹ç›®",
        entities=["ç§»åŠ¨åº”ç”¨", "å¼€å‘"],
        tags=["é¡¹ç›®", "ç§»åŠ¨ç«¯", "é‡è¦"]
    )

    memories.append({
        "memory": "ç§»åŠ¨åº”ç”¨é¡¹ç›®æ­£åœ¨è¿›è¡Œä¸­ï¼Œé¢„è®¡3ä¸ªæœˆå®Œæˆï¼Œå›¢é˜Ÿæœ‰8ä¸ªäºº",
        "metadata": project_metadata
    })

    # è®°å¿†3ï¼šå·¥ä½œè®°å¿†
    work_metadata = TreeNodeTextualMemoryMetadata(
        user_id=mem_cube.config.user_id,
        type="procedure",
        source="conversation",
        confidence=85.0,
        memory_type="WorkingMemory",
        key="æœ¬å‘¨ä»»åŠ¡",
        tags=["ä»»åŠ¡", "æœ¬å‘¨"]
    )

    memories.append({
        "memory": "æœ¬å‘¨éœ€è¦å®Œæˆéœ€æ±‚åˆ†æã€åŸå‹è®¾è®¡ã€ä»¥åŠæŠ€æœ¯é€‰å‹",
        "metadata": work_metadata
    })

    # æ·»åŠ åˆ°MemCube
    mem_cube.text_mem.add(memories)

    print("âœ… æˆåŠŸæ·»åŠ äº†3ä¸ªè®°å¿†é¡¹åˆ°MemCube")

    # æŸ¥è¯¢è®°å¿†
    print("\nğŸ” æŸ¥è¯¢æ‰€æœ‰è®°å¿†:")
    all_memories = mem_cube.text_mem.get_all()
    for i, memory in enumerate(all_memories, 1):
        print(f"{i}. {memory.memory}")
        print(f"   é”®: {memory.metadata.key}")
        print(f"   ç±»å‹: {memory.metadata.memory_type}")
        print(f"   æ ‡ç­¾: {memory.metadata.tags}")
        print()

    # æœç´¢ç‰¹å®šè®°å¿†
    print("ğŸ” æœç´¢åŒ…å«'æå››'çš„è®°å¿†:")
    search_results = mem_cube.text_mem.search("æå››", top_k=2)
    for result in search_results:
        print(f"- {result.memory}")
    
    return mem_cube

if __name__ == "__main__":
    create_structured_memories_ollama() 