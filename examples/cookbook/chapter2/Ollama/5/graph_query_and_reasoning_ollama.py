# graph_query_and_reasoning_ollama.py
# ğŸ¯ å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†ç¤ºä¾‹ (Ollamaç‰ˆ)
import os
from dotenv import load_dotenv
from memos.configs.memory import TreeTextMemoryConfig
from memos.memories.textual.tree import TreeTextMemory

def graph_query_and_reasoning_ollama():
    """
    ğŸ¯ å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†ç¤ºä¾‹ (Ollamaç‰ˆ)
    """
    
    print("ğŸš€ å¼€å§‹å›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç† (Ollamaç‰ˆ)...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥Ollamaé…ç½®
    ollama_base_url = os.getenv("OLLAMA_BASE_URL")
    ollama_chat_model = os.getenv("OLLAMA_CHAT_MODEL")
    ollama_embed_model = os.getenv("OLLAMA_EMBED_MODEL")
    
    if not ollama_base_url or not ollama_chat_model or not ollama_embed_model:
        raise ValueError("âŒ æœªé…ç½®Ollamaç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®OLLAMA_BASE_URLã€OLLAMA_CHAT_MODELã€OLLAMA_EMBED_MODELã€‚")
    
    print("âœ… æ£€æµ‹åˆ°Ollamaæœ¬åœ°æ¨¡å‹æ¨¡å¼")
    
    # è·å–ç”¨æˆ·ID
    user_id = os.getenv("MOS_USER_ID", "default_user")
    
    # åˆ›å»ºTreeTextMemoryé…ç½®
    tree_config = TreeTextMemoryConfig(
        extractor_llm={
            "backend": "ollama",
            "config": {
                "model_name_or_path": ollama_chat_model,
                "api_base": ollama_base_url
            }
        },
        dispatcher_llm={
            "backend": "ollama",
            "config": {
                "model_name_or_path": ollama_chat_model,
                "api_base": ollama_base_url
            }
        },
        graph_db={
            "backend": "neo4j",
            "config": {
                "uri": "bolt://localhost:7687",
                "user": "neo4j",
                "password": "password",
                "db_name": f"{user_id}_reasoning_memory",
                "auto_create": True,
                "embedding_dimension": 768
            }
        },
        embedder={
            "backend": "ollama",
            "config": {
                "model_name_or_path": ollama_embed_model,
                "api_base": ollama_base_url
            }
        }
    )
    
    # åˆ›å»ºTreeTextMemoryå®ä¾‹
    tree_memory = TreeTextMemory(tree_config)
    
    print("ğŸ” æ‰§è¡Œå›¾æ•°æ®åº“æŸ¥è¯¢å’Œæ¨ç†...")
    
    # 1. å‘é‡ç›¸ä¼¼åº¦æœç´¢
    print("\n1ï¸âƒ£ å‘é‡ç›¸ä¼¼åº¦æœç´¢:")
    vector_results = tree_memory.search("AIé¡¹ç›®", top_k=3)
    for i, result in enumerate(vector_results, 1):
        print(f"   {i}. {result.memory}")
    
    # 2. è·å–æ‰€æœ‰è®°å¿†
    print("\n2ï¸âƒ£ è·å–æ‰€æœ‰è®°å¿†:")
    all_memories = tree_memory.get_all()
    print(f"   æ€»è®°å¿†æ•°é‡: {len(all_memories.get('nodes', []))}")
    
    # 3. æ›¿æ¢å·¥ä½œè®°å¿†
    print("\n3ï¸âƒ£ æ›¿æ¢å·¥ä½œè®°å¿†:")
    new_working_memories = [{
        "memory": "å½“å‰æ­£åœ¨è¿›è¡Œéœ€æ±‚åˆ†æé˜¶æ®µï¼Œéœ€è¦æ”¶é›†ç”¨æˆ·åé¦ˆ",
        "metadata": {
            "memory_type": "WorkingMemory",
            "key": "å½“å‰çŠ¶æ€",
            "tags": ["çŠ¶æ€", "å½“å‰"]
        }
    }]
    tree_memory.replace_working_memory(new_working_memories)
    print("   âœ… å·¥ä½œè®°å¿†å·²æ›´æ–°")
    
    # 4. å¤‡ä»½è®°å¿†
    print("\n4ï¸âƒ£ å¤‡ä»½è®°å¿†åˆ°æ–‡ä»¶:")
    backup_dir = "tmp/tree_memory_backup"
    tree_memory.dump(backup_dir)
    print(f"   âœ… è®°å¿†å·²å¤‡ä»½åˆ°: {backup_dir}")
    
    print(f"\nğŸ¯ é…ç½®æ¨¡å¼: OLLAMA")
    print(f"ğŸ¤– èŠå¤©æ¨¡å‹: {ollama_chat_model}")
    print(f"ğŸ” åµŒå…¥æ¨¡å‹: {ollama_embed_model}")
    
    return tree_memory

if __name__ == "__main__":
    graph_query_and_reasoning_ollama() 